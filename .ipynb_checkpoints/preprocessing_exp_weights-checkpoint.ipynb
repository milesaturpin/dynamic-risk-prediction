{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_suffix_dict, generate_prefix_dict, to_json\n",
    "from train_test_split import train_val_test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from time import time\n",
    "from scipy.stats import linregress\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "proc_path = 'data/processed/'\n",
    "\n",
    "encounters = pd.read_pickle(proc_path + 'encounters.pickle')\n",
    "\n",
    "def merge_encounters(df):\n",
    "    '''\n",
    "    \n",
    "    Resulting dataframe is (656726, 29) with index (mrn, id);\n",
    "    time is not standardized.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Encounters contains 6500 unique encounters\n",
    "    enc = (\n",
    "        encounters[encounters.sbo_poa]\n",
    "        [['adm_datetime', 'any_sbo_surg', 'age',\n",
    "          'surg_datetime', ]]\n",
    "        .add_suffix('_enc')\n",
    "    )\n",
    "    merged_df = pd.merge(\n",
    "        enc, df.reset_index(level=2),\n",
    "        left_index=True, right_index=True,\n",
    "        how='inner'\n",
    "    )\n",
    "    merged_df['any_sbo_surg_enc'] = merged_df['any_sbo_surg_enc'].astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def standardize_times(df):\n",
    "\n",
    "    start = time()\n",
    "    df['min_datetime'] = (\n",
    "        df.datetime\n",
    "        .groupby(level=[0,1])\n",
    "        .transform(lambda x: x.min())\n",
    "    )\n",
    "    to_hour = lambda td: td.total_seconds() // 3600\n",
    "    df['hour_since_adm'] = (\n",
    "        (df.datetime - df.min_datetime)\n",
    "        .transform(to_hour)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    nonsurg_cutoff = 21*24\n",
    "    df['max_datetime_hour'] = (\n",
    "        df.hour_since_adm\n",
    "        .groupby(level=[0,1])\n",
    "        .transform(lambda x: min(nonsurg_cutoff, x.max()))\n",
    "    )\n",
    "    \n",
    "    df['event_hour_enc'] = (\n",
    "        (df.surg_datetime_enc - df.min_datetime)\n",
    "        .transform(to_hour)\n",
    "        .fillna(df['max_datetime_hour'])\n",
    "    )\n",
    "    \n",
    "    df['time_to_event_enc'] = (\n",
    "        df.event_hour_enc - df.hour_since_adm - 1\n",
    "    )\n",
    "    \n",
    "    df['time_of_day'] = (\n",
    "        df.hour_since_adm + df.min_datetime.apply(lambda dt: dt.hour)\n",
    "    )\n",
    "    \n",
    "    df = filter_populations(df, surg_cutoff=21*24)\n",
    "    \n",
    "    # Filter out measurements after surgery\n",
    "    df = df[df.hour_since_adm < df.event_hour_enc]\n",
    "    \n",
    "    df = df.drop(['datetime', 'min_datetime', \n",
    "                  'max_datetime_hour', 'surg_datetime_enc',\n",
    "                 'adm_datetime_enc', 'event_hour_enc'], 1)\n",
    "    \n",
    "    col_dict = generate_suffix_dict(df)\n",
    "    \n",
    "    mean_columns = (\n",
    "        df.reset_index()\n",
    "        .loc[:, col_dict['vitals'] + col_dict['labs'] + ['mrn', 'id','hour_since_adm']]\n",
    "        .groupby(['mrn', 'id','hour_since_adm'])\n",
    "        .agg('mean')\n",
    "    )\n",
    "    \n",
    "    sum_columns = (\n",
    "        df.reset_index().loc[:, col_dict['io'] + col_dict['occ'] + ['mrn', 'id','hour_since_adm']]\n",
    "        .groupby(['mrn', 'id','hour_since_adm'])\n",
    "        .agg('sum')\n",
    "    )\n",
    "    \n",
    "    enc = (\n",
    "        df.reset_index()[col_dict['enc'] + ['mrn', 'id','hour_since_adm']]\n",
    "        .groupby(['mrn', 'id','hour_since_adm'])\n",
    "        .agg('max')\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f'Finished standardizing times in {round(time()-start)}s')\n",
    "    return pd.concat([enc, mean_columns, sum_columns], axis=1)\n",
    "\n",
    "def filter_populations(df, surg_cutoff):\n",
    "    ''' '''\n",
    "    print('\\nBefore')\n",
    "    print(\"Number of patients: \" + str(df.reset_index().mrn.nunique()))\n",
    "    print('Number of encounters: ' + str(df.reset_index()['id'].nunique()))\n",
    "    print(df.reset_index()[['id', 'any_sbo_surg_enc']].drop_duplicates().any_sbo_surg_enc\n",
    "                .value_counts().transform(lambda x: x/x.sum()))\n",
    "\n",
    "    df = (df.groupby(level=[0,1])\n",
    "          .filter(\n",
    "              lambda df: not ((df.event_hour_enc > surg_cutoff) & (df.any_sbo_surg_enc == 1)).all()\n",
    "    ))\n",
    "    \n",
    "    print('\\nAfter')\n",
    "    print(\"Number of patients: \" + str(df.reset_index().mrn.nunique()))\n",
    "    print('Number of  encounters: '+ str(df.reset_index()['id'].nunique()))\n",
    "    print(df.reset_index()[['id', 'any_sbo_surg_enc']].drop_duplicates().any_sbo_surg_enc\n",
    "            .value_counts().transform(lambda x: x/x.sum()))\n",
    "        \n",
    "    return df\n",
    "\n",
    "def fix_occurrences(df):\n",
    "    col_dict = generate_suffix_dict(df)\n",
    "    df.loc[:, col_dict['occ']] = (\n",
    "        df[col_dict['occ']]            \n",
    "        .fillna(0).astype(bool).astype(int)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def create_rolling_matrix(data, window):\n",
    "    \"\"\"\n",
    "    Return rolling matrix of shape (m, n, window).\n",
    "    Pad with zeros so that we don't shrink.\n",
    "    \"\"\"\n",
    "    n, m = data.shape\n",
    "    buffer = np.zeros((window-1, m))\n",
    "    s2 = np.concatenate([buffer, data])\n",
    "    rolling_matrix = np.empty((m, n, window))\n",
    "    for i in range(n):\n",
    "        start = i\n",
    "        end   = i + window\n",
    "        # (m,0,window) <- (window,m).T\n",
    "        rolling_matrix[:,i,:] = s2[start:end].T\n",
    "    return rolling_matrix\n",
    "\n",
    "def expsum(data, zerolifes):\n",
    "    \"\"\"\n",
    "    Vectorized form of exponentially weighted sum.\n",
    "    We truncate the exponential sum when the weight\n",
    "    reaches 0.05 and we parameterize by the number of\n",
    "    steps to get there, the \"zerolife\". \n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    data (pd.DataFrame) -- df of size (n,m)\n",
    "    zerolifes (list) -- list of length z,\n",
    "        a zerolife of 3 will have a window_len of 3+1\n",
    "    \n",
    "    Return dataframe of size (n, m*z)\n",
    "    \"\"\"\n",
    "    z = len(zerolifes)\n",
    "    n, m = data.shape\n",
    "    # Take max of zerolifes + 1 to get window length\n",
    "    max_zerolife = max(zerolifes)\n",
    "    window_len   = max(zerolifes) + 1\n",
    "    rolling_matrix = create_rolling_matrix(data.values, window_len)\n",
    "    \n",
    "    # Create matrix of shape (window_len, z)\n",
    "    # Buffer with 0 to accommodate diff win lengths  \n",
    "    expvals = np.empty((window_len, z))\n",
    "    for zi, zerolife in enumerate(zerolifes):\n",
    "        buffer_len = max_zerolife - zerolife\n",
    "        expvals[:,zi] = np.array(\n",
    "            [0] * buffer_len  \n",
    "            + [ 0.05**(t/zerolife) for t in np.arange(zerolife + 1) ][::-1]\n",
    "        )\n",
    "\n",
    "    # (m, n, window_len) @ (window_len, z) -> (m, n, z)\n",
    "    expsum = rolling_matrix @ expvals\n",
    "    # Return (n, m*z), flatten multiple computations to 2D\n",
    "    df = pd.DataFrame(np.swapaxes(expsum, 0, 1).reshape((n, m*z)), index=data.index)\n",
    "    df.columns = [f'ems{zerolife}_' + col  for zerolife in zerolifes for col in data]\n",
    "    return df\n",
    "    \n",
    "def summary_stats(df):\n",
    "\n",
    "    col_dict = generate_suffix_dict(df)\n",
    "\n",
    "    max_hour = df.reset_index().hour_since_adm.max()\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    curr = df_copy[col_dict['vitals'] + col_dict['labs'] + col_dict['io'] + col_dict['occ']].add_prefix('curr_')\n",
    "    enc = df_copy[col_dict['enc']]\n",
    "    \n",
    "    df = df.reset_index(level=[0,1], drop=True).reindex(np.arange(max_hour + 1))\n",
    "    \n",
    "        \n",
    "    # Optimized tsl\n",
    "    io_df  = df[col_dict['io'] + col_dict['occ']]\n",
    "    num_df = df[col_dict['vitals'] + col_dict['labs']]\n",
    "    io_nan_mask = (io_df.fillna(0) == 0).astype(int).values\n",
    "    num_nan_mask = num_df.isna().astype(int).values\n",
    "    io_tsl_arr  = np.zeros((io_df.shape[0], io_df.shape[1]))\n",
    "    num_tsl_arr = np.zeros((num_df.shape[0], num_df.shape[1]))\n",
    "    for i in range(io_df.shape[0]):\n",
    "        io_tsl_arr[i,:]  = (1 + io_tsl_arr[i-1,:])*io_nan_mask[i,:]\n",
    "        num_tsl_arr[i,:] = (1 + num_tsl_arr[i-1,:])*num_nan_mask[i,:]\n",
    "        \n",
    "    io_df.loc[:,:] = io_tsl_arr\n",
    "    num_df.loc[:,:] = num_tsl_arr\n",
    "    \n",
    "    \n",
    "    ems = pd.concat(\n",
    "        (expsum(df[col_dict['io'] + col_dict['occ']].fillna(0), zerolifes = [z] ) \n",
    "        for z in [6,24,72]), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    ema_vitals = pd.concat(\n",
    "        (df[col_dict['vitals']].ewm(halflife=halflife).mean().add_prefix(f'ema{halflife}_')\n",
    "         for halflife in [6, 24, 72]), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    ema_labs = pd.concat(\n",
    "        (df[col_dict['labs']].ewm(halflife=halflife).mean().add_prefix(f'ema{halflife}_') \n",
    "         for halflife in [12, 48, 144]), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add variance?\n",
    "    \n",
    "    return pd.merge(\n",
    "        pd.concat([enc, curr],axis=1), \n",
    "        pd.concat([ema_vitals, ema_labs, ems, \n",
    "                   num_df.add_prefix('tsl_'),\n",
    "                   io_df.add_prefix('tsl_')], axis=1),\n",
    "        left_index=True, right_index=True, how='left'\n",
    "    )\n",
    "    \n",
    "def fill_na(df, train_means):\n",
    "    \"\"\"\n",
    "    PROTOCOL\n",
    "    Curr - ffill then fill with training set means\n",
    "    EMA - mean (should only be at beginning of encounter)\n",
    "    EMS - 0\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    pref_dict = generate_prefix_dict(df)\n",
    "    \n",
    "    df.loc[:,pref_dict['curr']] = (\n",
    "        df[pref_dict['curr']]\n",
    "        .fillna(method='ffill')\n",
    "        .fillna(train_means.loc[pref_dict['curr']])\n",
    "    )\n",
    "    \n",
    "    df.loc[:,pref_dict['ema']] = (\n",
    "        df[pref_dict['ema']]\n",
    "        .fillna(train_means.loc[pref_dict['ema']])\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def pipe_print(df,stage):\n",
    "    print('\\n')\n",
    "    print(f'>>> PIPE PRINT: {stage}')\n",
    "    print('Shape: ' + str(df.shape))\n",
    "    print('Number of Patients: ' + str(df.reset_index().mrn.nunique()))\n",
    "    print('Number of Encounters: '+ str(df.reset_index().id.nunique()))\n",
    "    return df\n",
    "\n",
    "def preprocess_exp_weights(\n",
    "        rebuild=False, \n",
    "        testing=False, \n",
    "        time_to_event=False, \n",
    "        time_varying=False):\n",
    "    '''\n",
    "    rebuild -- Rebuild the dataframe and save it\n",
    "    testing -- Use less data, less columns, and don't overwrite saved files\n",
    "    time_to_event -- Returns dataset with surgery indicator, as well as \n",
    "                     surg_datetime_hour_enc indicator\n",
    "    time_varying -- Returns dataset with surgery indicator only for time of surgery\n",
    "    ''' \n",
    "    \n",
    "    # Print out flag values\n",
    "    print('FLAGS')\n",
    "    print('Rebuild: ' + str(rebuild))\n",
    "    print('Testing: ' + str(testing))\n",
    "    print('Time to Event: ' + str(time_to_event))\n",
    "    print('Time Varying: ' + str(time_varying))\n",
    "\n",
    "    start = time()\n",
    "    \n",
    "    if rebuild:\n",
    "        print('\\n>>> Rebuilding file...')\n",
    "\n",
    "        if testing:\n",
    "            sbo = (pd.read_pickle(proc_path + 'sbo_mini.pickle')\n",
    "                   [['pulse_vitals', 'sodium_labs', \n",
    "                     'tube_output_io', 'stool_occ']])\n",
    "            epsilon = 0.05\n",
    "        else:\n",
    "            sbo = (pd.read_pickle(proc_path + 'sbo.pickle')\n",
    "                   .drop(['drain_output_io', 'ip_blood_administration_volume_io',\n",
    "                      'maintenance_iv_bolus_volume_io', 'rectal_tube_output_io'], 1))\n",
    "            epsilon = 0.01\n",
    "            \n",
    "        # Initial processing\n",
    "        sbo_presummary = (sbo\n",
    "            .pipe(pipe_print, 'Merge Encounters')\n",
    "            .pipe(merge_encounters)\n",
    "            .pipe(pipe_print, 'Standardize Times')\n",
    "            .pipe(standardize_times)\n",
    "            .pipe(pipe_print, 'Summary Stats')\n",
    "            .pipe(fix_occurrences)\n",
    "        )\n",
    "        \n",
    "        #return sbo_presummary\n",
    "        # Write out file before doing summary stats\n",
    "        if not testing:\n",
    "            sbo_presummary.to_pickle('data/processed/sbo_exp_presumm_full.pickle')\n",
    "        else:\n",
    "            sbo_presummary.to_pickle('data/processed/sbo_exp_presumm_mini_full.pickle')\n",
    "            \n",
    "        # Main processing\n",
    "        sbo_exp_weights = (\n",
    "            sbo_presummary\n",
    "            .groupby(level=[0,1])\n",
    "            .apply(lambda df: summary_stats(df))\n",
    "        )\n",
    "        \n",
    "        print('\\n>>> Train/val/test Split')\n",
    "        train, val, test, idx_dict = train_val_test(sbo_exp_weights, epsilon)\n",
    "        if not testing:\n",
    "            to_json(idx_dict, 'references/idx_dict_exp_weights.json')\n",
    "            \n",
    "        train_means = train.mean(axis=0)\n",
    "        train = fill_na(train, train_means)\n",
    "        val   = fill_na(val, train_means)\n",
    "        test  = fill_na(test, train_means)\n",
    "        \n",
    "        # Scale data\n",
    "        train, val, test = scale(train, val, test)\n",
    "        \n",
    "        # Write out preprocessed dataframe for visualization\n",
    "        if testing:\n",
    "            sbo_exp_weights.to_pickle('data/processed/sbo_exp_weights_mini.pickle')\n",
    "            train.to_pickle('data/processed/train_exp_weights_mini.pickle')\n",
    "            val.to_pickle('data/processed/val_exp_weights_mini.pickle')\n",
    "            test.to_pickle('data/processed/test_exp_weights_mini.pickle')\n",
    "        else:\n",
    "            sbo_exp_weights.to_pickle('data/processed/sbo_exp_weights.pickle')\n",
    "            train.to_pickle('data/processed/train_exp_weights.pickle')\n",
    "            val.to_pickle('data/processed/val_exp_weights.pickle')\n",
    "            test.to_pickle('data/processed/test_exp_weights.pickle')\n",
    "    else:\n",
    "        print('\\n>>> Reading in file...')\n",
    "        if testing:\n",
    "            train = pd.read_pickle('data/processed/train_exp_weights_mini.pickle')\n",
    "            val   = pd.read_pickle('data/processed/val_exp_weights_mini.pickle')\n",
    "            test  = pd.read_pickle('data/processed/test_exp_weights_mini.pickle')\n",
    "        else:\n",
    "            train = pd.read_pickle('data/processed/train_exp_weights.pickle')\n",
    "            val   = pd.read_pickle('data/processed/val_exp_weights.pickle')\n",
    "            test  = pd.read_pickle('data/processed/test_exp_weights.pickle')  \n",
    "    # TODO: Decide if filtering out 90+ \n",
    "    #print('FILTERING OUT 90+')\n",
    "    #train48, val48, test48 = (df[df.age_enc < 90] for df in [train48, val48, test48])\n",
    "    # Depending on goal, return different columns\n",
    "    \n",
    "    train48['surg_enc'] = (train48['any_sbo_surg_enc'].astype(bool) & \n",
    "                          (train48['time_to_event_enc'] == 0)).astype(int)\n",
    "    val48['surg_enc']   = (val48['any_sbo_surg_enc'].astype(bool) & \n",
    "                          (val48['time_to_event_enc'] == 0)).astype(int)\n",
    "    test48['surg_enc']  = (test48['any_sbo_surg_enc'].astype(bool) & \n",
    "                          (test48['time_to_event_enc'] == 0)).astype(int)\n",
    "    \n",
    "    if time_to_event:\n",
    "        cols_to_drop = ['any_sbo_surg_enc', 'surg_enc']\n",
    "    else:\n",
    "        cols_to_drop = ['any_sbo_surg_enc', 'surg_enc',\n",
    "                        'time_to_event_enc'\n",
    "                        ]\n",
    "                        \n",
    "    if time_varying:\n",
    "        surg_label = 'surg_enc'\n",
    "    else:\n",
    "        surg_label = 'any_sbo_surg_enc'\n",
    "    \n",
    "    # TODO: Decide if need to fill na for now because some values not in val set\n",
    "    x_train = train.drop(cols_to_drop, 1)\n",
    "    x_val   =   val.drop(cols_to_drop, 1)\n",
    "    x_test  =  test.drop(cols_to_drop, 1)\n",
    "    \n",
    "    y_train = train[surg_label]\n",
    "    y_val   =   val[surg_label]\n",
    "    y_test  =  test[surg_label]\n",
    "    x_cols = list(train48.drop(cols_to_drop,1).columns)\n",
    "    print('\\n\\n')\n",
    "    print('Finished processing in ' + str(round(time()-start)))\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test, x_cols\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x_train,y_train,x_val,y_val,x_test,y_test,x_cols = preprocess_exp_weights(\n",
    "        rebuild=True, \n",
    "        testing=True\n",
    "    )\n",
    "    #sbo_test = preprocess(rebuild=False, testing=True)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
